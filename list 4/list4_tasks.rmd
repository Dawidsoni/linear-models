---
title: "Linear models - Assignment 4"
author: Dawid Wegner
date: 25/01/2021
output: html_notebook
---

```{r}
library(ggplot2)
set.seed(100)
```

# Task 1

```{r}
round(qt(p = 0.975, 17), digits = 4)
```

We consider a linear model represented by the equation $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon$. Additionally, our parameters are estimated as $\hat{\beta_0} = 1$, $\hat{\beta_1} = 4$, $\hat{\beta_2} = 3$ and $s = 3$. As a consequence, it follows that for $X_1 = 2$, $X_2 = 6$ the predicted value is equal to $Y = 1 + 4 * 2 + 3 * 6 = 27$. The estimator of the variance of the prediction's error can be calculated as $\sigma^2_h(pred) = s^2 (1 + X_h (X^T X)^{-1} X_h^T) = s^2 + s^2_h = 9 + 4 = 13$, where we used the fact that $s_h = 2$. Lastly, the $95\%$ confidence interval can be estimated as $4.0 \pm t_c * s(\hat{\beta_1}) = 4.0 \pm 2.1098$.


# Task 2

```{r}
round(pf(1.5, df1 = 1, df2 = 20, lower.tail = FALSE), digits = 4)
```

```{r}
round(pf(1.5, df1 = 2, df2 = 20, lower.tail = FALSE), digits = 4)
```

```{r}
round(pf(6, df1 = 3, df2 = 20, lower.tail = FALSE), digits = 4)
```

```{r}
round(pf(14.3478, df1 = 1, df2 = 22, lower.tail = FALSE), digits = 4)
```

```{r}
round(sqrt(300 / 760), digits = 4)
```

To find type II sum of squares of the variable $X_3$, we can use the fact that type I sum of squares of the last variable is based on all previous variables i.e. $SSM(X_3 | X_1, X_2) = 20$. In particular, it is equal to type II sum of squares for $X_3$.

To test a hypothesis that $\beta_1 = 0$, we need to calculate $SSE(R) - SSE(F) = SSM(X_1 | X_2, X_3)$, $dfE(R) - dfE(F)$ and $MSE(F)$. Firstly, we can use a table to find that $SSM(X_1 | X_2, X_3) = 30$. The variable $SSE(F)$ can be calculated as $SST - SSM(F) = SST - SSM(X_1) - SSM(X_2 | X_1) - SSM(X_3 | X_1, X_2) = 760 - 300 - 40 - 20 = 400$, so $MSE(F) = SSE(F) / dfE(F) = 400 / 20 = 20$. Lastly, $dfE(R) - dfE(F) = |\{X_1\}| = 1$. After substituting variables, we can calculate the statistic as $F = \frac{(SSE(R) - SSE(F)) / (dfE(R) - dfE(F))}{MSE(F)} = 1.5$ and degrees of freedom as $df1 = 1$, $df2 = 20$. It implies that a p-value is equal to $0.2349$ and we can not reject the null hypothesis.

To test a hypothesis that $\beta_2 = \beta_3 = 0$, we can use the results from the previous task. The only difference is that $SSE(R) - SSE(F) = SSM(X_2, X_3 | X_1) = SSM(X_1, X_2, X_3) - SSM(X_1) = 360 - 300 = 60$ and $dfE(R) - dfE(F) = |\{X_2, X_3\}| = 2$. It implies that $F = 1.5$ with $df1 = 2, df2 = 20$ degrees of freedom and a p-value is equal to $0.2472$. It means that we can not reject the null hypothesis.

A hypothesis $\beta_1 = \beta_2 = \beta_3 = 0$ can be tested by substituting $SSE(R) - SSE(F) = SST - SSE(F) = 760 - 400 = 360$ and $dfE(R) - dfE(F) = |\{X_1, X_2, X_3\}| = 3$. It follows that $F = 6$ with $df1 = 3, df2 = 20$ degrees of freedom. As a result, a p-value is equal to $0.0044$, so we can reject the null hypothesis.

In this scenario, $SSE = 760 - 300 = 460$, $dfE(F) = 24 - 2 = 22$, implying that $MSE(F) \approx 20.9091$. As the next step, we can calculate $MSM(X_1) = SSM(X_1) = 300$. It implies that $F = \frac{MSM}{MSE} = \frac{300}{20.9091} = 14.3478$, so a p-value is equal to $0.001$. Based on this statistic, we can reject the null hypothesis.

The sample correlation between $X_1$ and $Y$ is a square root of $R^2_{X_1} = \frac{SSM(X1)}{SST}$. It implies that $corr(X_1, Y) = 0.6283$.


# Task 3

```{r}
transform_vectors_from_standard_norm <- function(vectors, mean_matrix, covariance_matrix) {
  a_matrix <- t(chol(covariance_matrix))
  b_matrix <- matrix(rep(mean_matrix, dim(vectors)[1]), nrow=dim(vectors)[2])
  transformed_vectors <- t(a_matrix %*% t(vectors) + b_matrix)
  return(transformed_vectors)
}


fit_and_explore_model_manually <- function (features_matrix, real_outputs, samples_count, df) {
  design_matrix <- cbind(rep(1, samples_count), features_matrix)
  estimated_betas <- solve(t(design_matrix) %*% design_matrix) %*% t(design_matrix) %*% real_outputs
  errors <- real_outputs - design_matrix %*% estimated_betas
  mse <- sum(errors ^ 2) / df
  beta1_var <- mse * solve(t(design_matrix) %*% design_matrix)[2, 2]
  t_statistic <- estimated_betas[2] / sqrt(beta1_var)
  estimated_p_value <- 2 * (1 - pt(abs(t_statistic), df))
  print(paste0("Estimated standard deviation: ", round(sqrt(beta1_var) ,digits = 4)))
  print(paste0("Estimated t-statistic: ", round(t_statistic, digits = 4)))
  print(paste0("Estimated p-value: ", round(estimated_p_value, digits = 4)))
}


explore_estimator_variance <- function (
  data_matrix, features_matrix, beta1, samples_count, df, simulations_count = 1000, significance_level = 0.05
) {
  design_matrix <- cbind(rep(1, samples_count), features_matrix)
  beta1_values <- rep(NA, simulations_count)
  null_hypothesis_rejection_count <- 0
  for (i in 1:simulations_count) {
    real_outputs <- beta1 * data_matrix[, 1]  + rnorm(samples_count)
    estimated_betas <- solve(t(design_matrix) %*% design_matrix) %*% t(design_matrix) %*% real_outputs
    errors <- real_outputs - design_matrix %*% estimated_betas
    mse <- sum(errors ^ 2) / df
    beta1_var <- mse * solve(t(design_matrix) %*% design_matrix)[2, 2]
    t_statistic <- estimated_betas[2] / sqrt(beta1_var)
    estimated_p_value <- 2 * (1 - pt(abs(t_statistic), df))
    if (estimated_p_value <= significance_level) {
      null_hypothesis_rejection_count <- null_hypothesis_rejection_count + 1
    }
    beta1_values[[i]] <- estimated_betas[2]
  }
  print(paste0("Standard deviation of beta1 estimators: ", round(sd(beta1_values), digits = 4)))
  print(paste0("Power of a test: ", round(null_hypothesis_rejection_count / simulations_count, digits = 4)))
}


explore_correlation_influence <- function (beta1) {
  features_matrix <- transform_vectors_from_standard_norm(
    vectors = matrix(rnorm(200), ncol = 2),
    mean_matrix = matrix(c(0.0, 0.0), nrow=2),
    covariance_matrix = matrix(c(1.0, 0.9, 0.9, 1.0) / 100.0, nrow=2)
  )
  feature1_vector <- features_matrix[, 1]
  feature2_vector <- features_matrix[, 2]
  real_outputs <- beta1 * feature1_vector  + rnorm(100)
  real_model <- lm(real_outputs ~ feature1_vector)
  extended_model <- lm(real_outputs ~ feature1_vector + feature2_vector)
  ci_of_real_model <- round(confint(real_model, 'feature1_vector', level = 0.95), digits = 4)
  ci_of_extended_model <- round(confint(extended_model, 'feature1_vector', level = 0.95), digits = 4)
  print(paste0("Real model confidence interval: [", ci_of_real_model[1], ", ", ci_of_real_model[2], "]"))
  print(paste0("Extended model confidence interval: [", ci_of_extended_model[1], ", ", ci_of_extended_model[2], "]"))
  pvalue_of_real_model <- round(summary(real_model)[["coefficients"]][2, 4], digits = 4)
  pvalue_of_extended_model <- round(summary(extended_model)[["coefficients"]][2, 4], digits = 4)
  print(paste0("P-value for real model: ", pvalue_of_real_model))
  print(paste0("P-value for extended model: ", pvalue_of_extended_model))
  print("Exploring properties of real model")
  fit_and_explore_model_manually(feature1_vector, real_outputs, samples_count = 100, df = 98)
  print("Exploring properties of extended model")
  fit_and_explore_model_manually(features_matrix, real_outputs, samples_count = 100, df = 98)
  print("Exploring variance of real model")
  explore_estimator_variance(features_matrix, feature1_vector, beta1, samples_count = 100, df = 98)
  print("Exploring variance of extended model")
  explore_estimator_variance(features_matrix, features_matrix, beta1, samples_count = 100, df = 98)
}


explore_correlation_influence(beta1 = 3)
```

The first observation is that the confidence interval for the $\beta_1$ parameter increased in length when additional variable was introduced. It is caused by the fact that the second variable is highly correlated with the first one, implying that some computational and statistical problems can occur. The other side effect is that a p-value for the 2nd model is higher on average. Further analysis also shows that a standard deviation of $\beta_1$ estimator is much higher in the 2nd model compared to the 1st model. After simulating the process of estimating $\beta_1$ parameter $1000$ times, it is clearly visible that a standard deviation of $\beta_1$ estimation is much higher in the 2nd model. Additionally, the power of a test dropped significantly in the 2nd model from $0.857$ to $0.221$.


# Task 4

```{r}
calculate_aic_statistic <- function (model) {
  samples_count <- length(model$residuals)
  variables_count <- length(model$coefficients)
  sse <- sum(model$residuals ^ 2)
  return(samples_count * log(sse / samples_count) + 2 * variables_count)
}


construct_and_explore_model <- function (design_matrix, real_coefficients, real_outputs) {
  output_df <- data.frame(matrix(nrow = 1, ncol = 7))
  colnames(output_df) <- c("cols_count", "sse", "mse", "aic", "p_value1", "p_value2", "false_discoveries_count")
  samples_df <- data.frame(x = design_matrix, y = real_outputs)
  model <- lm(y ~ . -1, data = samples_df)
  output_df$cols_count <- dim(design_matrix)[2]
  output_df$sse <- sum(model$residuals ^ 2)
  output_df$mse <- sum((design_matrix %*% (model$coefficients - real_coefficients)) ^ 2)
  output_df$aic <- calculate_aic_statistic(model)
  p_values <- as.matrix(summary(model)$coefficients[, 4])
  output_df$p_value1 <- p_values[1]
  output_df$p_value2 <- p_values[2]
  is_null_hypothesis_true <- real_coefficients != 0
  output_df$false_discoveries_count <- sum((p_values < 0.05) != is_null_hypothesis_true)
  return(output_df)
}


explore_impact_of_multiple_dimensions <- function (
  samples_count, explanatory_variables_count, dimensions, reverse_columns = FALSE
) {
  output_df <- data.frame()
  design_matrix <- matrix(rnorm(samples_count * explanatory_variables_count, mean = 0.0, sd = 0.1), nrow = 1000)
  beta_variables <- c(rep(3.0, 5), rep(0.0, explanatory_variables_count - 5))
  random_errors <- rnorm(samples_count)
  real_outputs <- design_matrix %*% beta_variables + random_errors
  max_cols_count <- dim(design_matrix)[2]
  for (cols_count in dimensions) {
    columns_subset <- if (reverse_columns) max_cols_count:(max_cols_count - cols_count + 1) else 1:cols_count
    output_df <- rbind(output_df, construct_and_explore_model(
      as.matrix(design_matrix[, columns_subset]), beta_variables[columns_subset], real_outputs
    ))
  }
  return(output_df)
}


explored_dimensions <- c(1, 2, 5, 10, 50, 100, 500, 950)
explored_dimensions_df <- explore_impact_of_multiple_dimensions(
  samples_count = 1000, explanatory_variables_count = 950, dimensions = explored_dimensions
)
round(explored_dimensions_df, digits = 4)
```

```{r}
explored_dimensions_df <- explore_impact_of_multiple_dimensions(
  samples_count = 1000, explanatory_variables_count = 950, dimensions = explored_dimensions, reverse_columns = TRUE
)
round(explored_dimensions_df, digits = 4)
```

In the first scenario, it can be observed that the AIC metric has the lowest value for the model with $950$ parameters, while we would expect this metric to have the lowest value for the model with $5$ parameters. On the other hand, we can see that the AIC metric has the second lowest value for the model with $5$ parameters, suggesting that the AIC metric works well for dimensions up to $500$. In the case of $950$ dimensions, it can be observed that its SSE is really low compared to other models. It means that the AIC metric does not provide enough penalty for very high dimensional data. The other insight from the experiment is that the number of false discoveries increases significantly when a dimension of data is increased.

In the second scenario, the model with $950$ parameters seems to be much better than other models. It is expected as all other models are based on some random noise. Particularly, the AIC metric increases when there are more random noise variables. Similarly to the previous case, the number of false discoveries significantly increased for models constructed in higher dimensions.


# Task 5

```{r}
patients_df <- read.csv("CH06PR15.txt", header = FALSE, sep = "")
colnames(patients_df) <- c("age", "severity_level", "anxiety_level", "satisfaction_level")
reduced_patients_model <- lm(satisfaction_level ~ 1, data = patients_df)
full_patients_model <- lm(satisfaction_level ~ age + severity_level + anxiety_level, data = patients_df)
full_patients_model
```

```{r}
summary(full_patients_model)
```

```{r}
round(anova(reduced_patients_model, full_patients_model), digits = 4)
```

The goal is to predict a level of $satisfaction$ based on patient's $age$, $severity$ of patient's illness and a level of patient's $anxiety$. The fitted model can be characterized by the equation: $satisfaction = 1.053245 -0.005861 * age + 0.001928 * severity + 0.030148 * anxiety$. The $R^2$ statistic is equal to $0.5415$, indicating that there is some linear dependence between the response variable and the explanatory variables, but it may be not strong enough to predict the level of $satisfaction$ accurately.

The null hypothesis of the conducted test states that $\beta_{age} = \beta_{severity} = \beta_{anxiety} = 0$, while the alternative hypothesis states that at least one of these estimators is not equal to $0$. The test statistic $F$ is equal to $16.5376$, which is a difference between the $SSEs$ of the reduced and full model, divided by the expression $3 * MSE(F)$. The test statistic has $df1 = dfE(R) - dfE(F) = 3$, $df2 = dfE(F) = 42$ degrees of freedom. From the $CDF$ of the $Fisher$ distribution, it follows that a p-value of the test is very close to $0.0$, meaning that we can reject the null hypothesis. In summary, there is some dependence between at least one of the explanatory variables and the response variable.

# Task 6

```{r}
print(summary(full_patients_model))
print(round(confint(full_patients_model, 'age', level = 0.95), digits = 3))
print(round(confint(full_patients_model, 'severity_level', level = 0.95), digits = 3))
print(round(confint(full_patients_model, 'anxiety_level', level = 0.95), digits = 3))
```

The $95\%$ confidence interval for the $age$ variable is equal to $[-0.012, 0.0]$, while the test statistic is equal to $T = -1.897$ with $n - p = 46 - 2 = 44$ degrees of freedom. It suggests that the null hypothesis of a test, stating that $\beta_{age} = 0$, may be true. In particular, a p-value of this test is equal to $0.06468$, indicating that the null hypothesis can not be rejected if we assume that a significance level is equal to $\alpha = 0.05$.

The confidence interval of the $severity$ variable is equal to $[-0.01, 0.014]$. The test statistic is equal to $T = 0.333$ with $n - p = 46 - 2 = 44$ degrees of freedom. Similarly to the previous case, it suggests that the null hypothesis, testing this parameter, can be true. After examining a p-value of this test, which is equal to $0.74065$, we can conclude that there is a high chance that there is no linear dependence between the $severity$ variable and the $satisfaction$ variable.

Lastly, the confidence interval for the $anxiety$ variable is equal to $[0.011, 0.049]$, the test statistic is equal to $3.257$, the number of degrees of freedom is equal to $n - p = 46 - 2 = 44$, which suggests that a p-value of the corresponding test will be low. The results are in line with our intuition as a p-value is equal to $0.00223$, which means that we can reject the null hypothesis.

In summary, there is enough evidence to believe that there is some relation between the level of $anxiety$ and the level of patient's $satisfaction$. In contrast, our tests do not provide enough evidence to believe that there is correspondence between the $age$, $severity$ variables and the $satisfaction$ variable.


# Task 7

```{r}
satisfactions_df <- data.frame(
  residual = full_patients_model$residuals,
  prediction = full_patients_model$fitted.values,
  age = patients_df$age,
  severity_level = patients_df$severity_level,
  anxiety_level = patients_df$anxiety_level
)
print(ggplot(satisfactions_df) +
    geom_point(aes(x = prediction, y = residual), shape = 19, size = 2.2, color = "dark orange") +
    xlab("Predicted value") +
    ylab("Residual") +
    ggtitle("Plot of predicted values versus residuals for the satisfaction model") +
    theme(plot.title = element_text(hjust = 0.5)))
print(ggplot(satisfactions_df) +
    geom_point(aes(x = age, y = residual), shape = 19, size = 2.2, color = "dark orange") +
    xlab("Age") +
    ylab("Residual") +
    ggtitle("Plot of peoples' ages versus residuals for the satisfaction model") +
    theme(plot.title = element_text(hjust = 0.5)))
print(ggplot(satisfactions_df) +
    geom_point(aes(x = severity_level, y = residual), shape = 19, size = 2.2, color = "dark orange") +
    xlab("Severity level") +
    ylab("Residual") +
    ggtitle("Plot of severity levels versus residuals for the satisfaction model") +
    theme(plot.title = element_text(hjust = 0.5)))
print(ggplot(satisfactions_df) +
    geom_point(aes(x = anxiety_level, y = residual), shape = 19, size = 2.2, color = "dark orange") +
    xlab("Anxiety level") +
    ylab("Residual") +
    ggtitle("Plot of anxiety levels versus residuals for the satisfaction model") +
    theme(plot.title = element_text(hjust = 0.5)))
```

The first plot compares residuals with predicted values. The main observation from the plot is that there are a few points for which a prediction error is much higher than for other points. In particular, there are two points with an error higher than $0.4$, while most of points have their error below $0.2$. What is more, predictions for these $2$ points are very close to each other, indicating that these outliers are somehow correlated. This intuition can be confirmed by looking into the next $3$ plots. In particular, it can be seen that these $2$ points as well as other outliers occur for people of age at least $70$, severity level below $35$ and anxiety level between $45$ and $50$. It means that either our model doesn't work well for values in this domain or these measurements were made incorrectly.


# Task 8

```{r}
print(ggplot(satisfactions_df, aes(sample = residual)) +
  ggtitle("QQ-plot of residuals versus standard normal distribution") +
  xlab("Theoretical value") +
  ylab("Residual") +
  geom_qq(colour="dark orange") +
  geom_qq_line(colour="blue") +
  theme(plot.title = element_text(hjust = 0.5)))
```

```{r}
shapiro.test(satisfactions_df$residual)
```

The QQ-plot suggests that while some residuals are outliers, the overall distribution of residuals is close to the normal distribution. In particular, the vast majority of points is very close to a fitted line. There are only a few points in the corners of the plot that are above the fitted line, which were already indicated as outliers in the previous plots.

The Shapiro-Wilk test is a test of normality based on order statistics of given samples. In particular, the test uses samples from the standard normal distribution and compare them with given samples, which are normalized. The null hypothesis states that given samples come from a normal distribution with some mean and variance. In our case, a p-value is equal to $0.1481$, which means that we can not reject the null hypothesis if we assume that a significance level is equal to $\alpha = 0.05$.

In summary, after plotting residuals as well as using the Sharipo-Wilk test, we have strong evidence that residuals come from a normal distribution. The other observation is that there are some outliers that should be explored more deeply.


# Task 9

```{r}
cs_df <- read.csv("csdata.txt", header = FALSE, sep = "")
colnames(cs_df) <- c("id", "gpa", "hsm", "hss", "hse", "satm", "satv", "sex")
reduced_gpa_model <- lm(gpa ~ hsm + hss + hse, data = cs_df)
full_gpa_model <- lm(gpa ~ hsm + hss + hse + satm + satv, data = cs_df)
reduced_gpa_sse <- sum(reduced_gpa_model$residuals ^ 2)
full_gpa_sse <- sum(full_gpa_model$residuals ^ 2)
f_statistic <- ((reduced_gpa_sse - full_gpa_sse) / 2) / (full_gpa_sse / (dim(cs_df)[1] - 6))
round(f_statistic, digits = 4)
```

```{r}
round(anova(reduced_gpa_model, full_gpa_model), digits = 4)
```

The theory states that the $F$ statistic is equal to the following expression: $F = \frac{(SSE(R) - SSE(F)) / (dfE(R) - dfE(F))}{MSE(F)}$. After calculating these quantities, we have established that $F = 0.9503$. This result was validated by the output of the $anova$ function.

The null hypothesis of the conducted test states that $\beta_{SATM} = \beta_{SATV} = 0$ in the full model. The alternative hypothesis states that at least one these estimators is not equal to $0$. The computed $F$ statistic has $df1 = 2$, $df2 = n - p = 224 - 6 = 218$ degrees of freedom. Lastly, a p-value is equal to $0.3882$, which indicates that we can not reject the null hypothesis.


# Task 10

```{r}
print_type2_sum_of_squares <- function (df, explanatory_variables, response_variable) {
  formula_of_full_model <- as.formula(
    paste(response_variable, paste(explanatory_variables, collapse=" + "), sep=" ~ ")
  )
  full_model <- lm(formula_of_full_model, data = df)
  for (variable_index in seq_along(explanatory_variables)) {
    chosen_explanatory_variables <- explanatory_variables[-variable_index]
    formula_of_reduced_model <- as.formula(
      paste(response_variable, paste(chosen_explanatory_variables, collapse=" + "), sep=" ~ ")
    )
    reduced_model <- lm(formula_of_reduced_model, data = df)
    print(round(anova(reduced_model, full_model), digits = 4))
  }
}


verify_hsm_type1_sum_of_squares <- function (df) {
  basic_model <- lm(gpa ~ satm + satv, data = df)
  extended_model <- lm(gpa ~ satm + satv + hsm, data = df)
  sse_of_basic_model <- sum(basic_model$residuals ^ 2)
  sse_of_extended_model <- sum(extended_model$residuals ^ 2)
  print(round(sse_of_basic_model - sse_of_extended_model, digits = 4))
}


sums_gpa_model <- lm(gpa ~ satm + satv + hsm + hse + hss, data = cs_df)
round(anova(sums_gpa_model), digits = 4)
```

```{r}
print_type2_sum_of_squares(
  cs_df, explanatory_variables = c("satm", "satv", "hsm", "hse", "hss"), response_variable = "gpa"
)
```

```{r}
verify_hsm_type1_sum_of_squares(cs_df)
```

The type I sums of squares can be calculated by calling the function $anova$ with a linear model, having variables in the right order. In our case, the outputs provide us the following information: $SSM(SATM) = 8.5829$, $SSM(SATV | SATM) = 0.0009$, $SSM(HSM | SATM, SATV) = 17.7265$ etc.

The type II sum of squares was calculated by calling the function $anova$ multiple times with a reduced model without a specific variable and a full model composed of all variables. The outputs provide us with the following information: $SSM(SATM | SATV, HSM, HSE, HSS) = 0.928$, $SSM(SATV | SATM, HSM, HSE, HSS) = 0.2327$ etc.

The first table shows that $SSM(HSM | SATM, SATV) = 17.7265$. The same value was obtained by taking the difference $SSE(HSM, SATM, SATV) - SSE(SATM, SATV)$. It is in line with the theory, stating that $SSM(HSM | SATM, SATV) = SSE(HSM | SATM, SATV) = SSE(HSM, SATM, SATV) - SSE(SATM, SATV)$.

The type I and type II sum of squares for the $HSS$ variable is equal to $0.4421$ in both cases. It is expected as this is the last variable, conditioning on the information collected by all other variables.

# Task 11

```{r}
cs_df$sat <- cs_df$satm + cs_df$satv
sat_gpa_model <- lm(gpa ~ satm + satv + sat, data = cs_df)
sat_gpa_model
```

```{r}
summary(sat_gpa_model)
```

The results indicate that there is a problem with the $SAT$ variable. In particular, a table containing values for a test $\beta_{SAT} = 0$ contains $NA$ values. The reason for this phenomenon is that the matrix $X^T X$ is singular. It implies that the inverse of this matrix is not well-defined and as a result $\hat{\beta} = (X^T X)^{-1} X^T Y$ is not well-defined. This is very undesirable property that is called a multicollinearity problem.


# Task 12

```{r}
plot_partial_regression <- function (df, explanatory_variables, examined_variable, response_variable) {
  response_variable_formula <- as.formula(
    paste(response_variable, paste(explanatory_variables, collapse=" + "), sep=" ~ ")
  )
  response_variable_model <- lm(response_variable_formula, data = df)
  examined_variable_formula <- as.formula(
    paste(examined_variable, paste(explanatory_variables, collapse=" + "), sep=" ~ ")
  )
  examined_variable_model <- lm(examined_variable_formula, data = df)
  residuals_df <- data.frame(
    response_variable = response_variable_model$residuals, examined_variable = examined_variable_model$residuals
  )
  print(ggplot(residuals_df, aes(x = response_variable, y = examined_variable)) +
    geom_point(shape = 19, size = 2.2, color = "dark orange") +
    geom_smooth(method = "lm", formula = y ~ x, se = FALSE) +
    xlab("Residuals of a model predicting 'GPA'") +
    ylab(paste0("Residuals of a model predicting '", examined_variable, "'")) +
    ggtitle(paste0("Partial regression plot of the '", examined_variable, "' variable")) +
    theme(plot.title = element_text(hjust = 0.5)))
}


create_partial_regressions_plots <- function (df, explanatory_variables, response_variable) {
  for (variable_index in seq_along(explanatory_variables)) {
    chosen_explanatory_variables <- explanatory_variables[-variable_index]
    examined_variable <- explanatory_variables[variable_index]
    plot_partial_regression(df, chosen_explanatory_variables, examined_variable, response_variable)
  }
}


create_partial_regressions_plots(
  cs_df,
  explanatory_variables = c("hsm", "hss", "hse", "satm", "satv", "sex"),
  response_variable = "gpa"
)
```

A partial regression plot examines the impact of one specific explanatory variable on the response variable, taking into account all other variables. In more detail, it plots residuals of a model predicting the response variable from all explanatory variables except the one that is examined against residuals of a model predicting the the examined variable based on other explanatory variables. If the examined variable adds some information to the model, there should be a linear relation (with slope not equal to $0$) between the plotted residuals.

The plots show that most of the variables don't add any extra information to the model if we use all the other variables. The only variable with a value of a slope far from $0$ is the $HSM$ variable. It is in line with our previous analysis, which showed that a p-value for the test $\beta_{HSM} = 0$ in the full model is close to $0.0$, while it is significantly higher than $\alpha = 0.05$ for other variables.


# Task 13

```{r}
plot_deleted_residuals <- function (df, explanatory_variables, response_variable) {
  df$intercept <- 1.0
  model_formula <- as.formula(
    paste(response_variable, paste(explanatory_variables, collapse=" + "), sep=" ~ ")
  )
  data_matrix <- data.matrix(df[c("intercept", c(explanatory_variables))])
  h_matrix <- data_matrix %*% solve(t(data_matrix) %*% data_matrix) %*% t(data_matrix)
  studentized_residuals <- rep(NA, dim(df)[1])
  for (index in seq(1, dim(df)[1])) {
    sample_discarded_df <- df[-index, ]
    sample_discarded_model <- lm(model_formula, data = sample_discarded_df)
    enumerator_value <- df[index, "gpa"] - predict(sample_discarded_model, df[index, ])
    denominator_value <- sqrt(sum((sample_discarded_model$residuals) ^ 2) * (1 - h_matrix[index, index]))
    studentized_residuals[index] <- enumerator_value / denominator_value
  }
  studentized_residuals_df <- data.frame(index = seq(1, length(studentized_residuals)), value = studentized_residuals)
  print(ggplot(studentized_residuals_df, aes(x = index, y = value)) +
    geom_point(shape = 19, size = 2.2, color = "dark orange") +
    xlab("Sample index") +
    ylab("Studentized deleted residual") +
    ggtitle("Plot of studentized deleted residuals of the GPA model") +
    theme(plot.title = element_text(hjust = 0.5)))
}


plot_deleted_residuals(
  cs_df,
  explanatory_variables = c("hsm", "hss", "hse", "satm", "satv", "sex"),
  response_variable = "gpa"
)
```

The plot of studentized deleted residuals shows that there are no points, which can be certainly marked as outliers. In particular, it is common to assume that a point is an outlier if an absolute value of studentized deleted residual is equal to at least $3$. In our case, all points lie in the range $[-0.25, 0.2]$.


# Task 14

```{r}
examine_dffits <- function (df, explanatory_variables, response_variable) {
  df$intercept <- 1.0
  model_formula <- as.formula(
    paste(response_variable, paste(explanatory_variables, collapse=" + "), sep=" ~ ")
  )
  full_model <- lm(model_formula, data = df)
  data_matrix <- data.matrix(df[c("intercept", c(explanatory_variables))])
  h_matrix <- data_matrix %*% solve(t(data_matrix) %*% data_matrix) %*% t(data_matrix)
  dffits_values <- rep(NA, dim(data_matrix)[1])
  for (index in seq(1, dim(data_matrix)[1])) {
    sample_discarded_df <- df[-index, ]
    sample_discarded_model <- lm(model_formula, data = sample_discarded_df)
    enumerator_value <- predict(full_model, df[index, ]) - predict(sample_discarded_model, df[index, ])
    sample_discarded_sse <- sum((sample_discarded_model$residuals) ^ 2)
    sample_discarded_mse <- sample_discarded_sse / (dim(data_matrix)[1] - dim(data_matrix)[2])
    denominator_value <- sqrt(sample_discarded_mse * h_matrix[index, index])
    dffits_values[index] <- enumerator_value / denominator_value
  }
  dffits_df <- data.frame(index = seq(1, length(dffits_values)), value = dffits_values)
  print(ggplot(dffits_df, aes(x = index, y = value)) +
    geom_point(shape = 19, size = 2.2, color = "dark orange") +
    xlab("Sample index") +
    ylab("DFFITS value") +
    ggtitle("Plot of DFFITS values of the GPA model") +
    theme(plot.title = element_text(hjust = 0.5)))
}


examine_dffits(
  cs_df,
  explanatory_variables = c("hsm", "hss", "hse", "satm", "satv", "sex"),
  response_variable = "gpa"
)
```

The DFFITS is a statistic that measures the impact of the $i-th$ observation on the $i-th$ prediction in the full model. The intuition behind this statistic is that the difference between the prediction in the full model and the prediction in the model without the $i-th$ sample should be low. Otherwise, the $i-th$ observation is either an outlier or an influential point. In particular, it is common to assume that the set of these points is determined by the condition $|DFFITS_i| > \sqrt{\frac{4p}{n}}$.

In our case, a threshold is equal to $\sqrt{\frac{4p}{n}} = 2 \sqrt{\frac{7}{224}} \approx 0.354$. The plot shows that there are about $10$ points for which the value of $DFFITS$ is significantly higher than this threshold.


# Task 15

```{r}
plot_tolerance_statistic <- function (df, explanatory_variables) {
  tolerances <- rep(NA, length(explanatory_variables))
  for (variable_index in seq_along(explanatory_variables)) {
    chosen_explanatory_variables <- explanatory_variables[-variable_index]
    response_variable <- explanatory_variables[variable_index]
    variable_formula <- as.formula(
      paste(response_variable, paste(chosen_explanatory_variables, collapse=" + "), sep=" ~ ")
    )
    variable_model <- lm(variable_formula, data = df)
    model_sse <- sum((variable_model$residuals) ^ 2)
    model_sst <- sum((df[[response_variable]] - mean(df[[response_variable]])) ^ 2)
    model_r_squared <- 1 - model_sse / model_sst
    tolerances[[variable_index]] <- 1 - model_r_squared
  }
  tolerances_df <- data.frame(variable = explanatory_variables, tolerance = tolerances)
  ggplot(tolerances_df, aes(x = variable, y = tolerance)) +
    geom_bar(stat = "identity", fill = "blue") +
    xlab("Variable name") +
    ylab("Tolerance") +
    ggtitle("Plot of the tolerance statistc for explanatory variables of the GPA model") +
    theme(plot.title = element_text(hjust = 0.5))
}


plot_tolerance_statistic(
  cs_df, explanatory_variables = c("hsm", "hss", "hse", "satm", "satv", "sex")
)
```

The tolerance is a measure of multicollinearity of explanatory variables. Its value is defined for each variable and lies in the range $[0, 1]$. The low values indicate that a variable can be represented as a linear combination of other variables. In particular, it is common to assume that the tolerance value below $0.1$ indicates that there are significant problems with the multicollinearity.

The plot shows that there is some relation between explanatory variables, but it is not strong enough to cause any computational or statistical problems. It only suggests that it may be beneficial to remove some variables from our model.


# Task 16

```{r}
calculate_bic_statistic <- function (model) {
  samples_count <- length(model$residuals)
  variables_count <- length(model$coefficients)
  sse <- sum(model$residuals ^ 2)
  return(samples_count * log(sse / samples_count) + log(samples_count) * variables_count)
}


create_and_rate_submodels <- function (df, explanatory_variables, response_variable) {
  ratings_df <- data.frame(matrix(nrow = 2 ^ length(explanatory_variables) - 1, ncol = 3))
  colnames(ratings_df) <- c("variables", "aic", "bic")
  index_of_model <- 1
  for (variables_count in seq_along(explanatory_variables)) {
    variables_subsets <- combn(explanatory_variables, variables_count, simplify = FALSE)
    for (chosen_explanatory_variables in variables_subsets) {
      formula_of_model <- as.formula(
        paste(response_variable, paste(chosen_explanatory_variables, collapse=" + "), sep=" ~ ")
      )
      model <- lm(formula_of_model, data = df)
      ratings_df$variables[index_of_model] <- paste(chosen_explanatory_variables, collapse=", ")
      ratings_df$aic[index_of_model] <- round(calculate_aic_statistic(model), digits = 2)
      ratings_df$bic[index_of_model] <- round(calculate_bic_statistic(model), digits = 2)
      index_of_model <- index_of_model + 1
    }
  }
  return(ratings_df)
}

cs_ratings_df <- create_and_rate_submodels(
  cs_df, explanatory_variables = c("hsm", "hss", "hse", "satm", "satv", "sex"), response_variable = "gpa"
)
cs_ratings_df
```

The AIC is a statistic that aims to help with picking the optimal model. Its goal is to minimize the expression $n log (\frac{SSE}{n}) + 2p$, where $n$ is the number of samples, $SSE$ is a sum squared error of the constructed model and $p$ is the penalty for the high number of parameters. Similarly, the BIC statistic minimizes the expression $n log (\frac{SSE}{n}) + log(n)p$. Both statistics are often used to find a balance between the fitness to the training data and the number of model's parameters.

As the number of variables is low, models with all possible subsets of variables were constructed. The AIC statistic indicates that the model containing $HSM$, $HSE$ variables is the best one, while the BIC statistic indicates that the only $HSM$ variable should be leveraged in the optimal model. These results are in line with the previous analysis, which already showed that new variables provide very little additional information.

