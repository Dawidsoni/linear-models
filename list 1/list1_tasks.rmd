---
title: "Linear models - Assignments 1"
author: Dawid Wegner
date: 07/10/2020
output: html_notebook
---

# Task 1

```{r}
plot_vectors <- function(vectors, title) {
  plot(vectors, xlab='X', ylab='Y', pch=16, col="blue", main=title)
}

vectors <- matrix(rnorm(n=200), nrow=100)
plot_vectors(vectors, title="Samples from N(0, I)")
```

Vectors from $N(0, Id)$ were generated by observing that random variables $X$ and $Y$ are independent. Moreover, these variables have the same variance, so generating 100 2-dimensional vectors corresponds to generating 200 scalar values.

# Task 2

```{r}
transform_vectors_from_standard_norm <- function(vectors, mean_matrix, covariance_matrix) {
  a_matrix <- t(chol(covariance_matrix))
  b_matrix <- matrix(rep(mean_matrix, dim(vectors)[1]), nrow=dim(vectors)[2])
  transformed_vectors <- t(a_matrix %*% t(vectors) + b_matrix)
  return(transformed_vectors)
}
```

Recall that it holds: $\Sigma^{Y} = A\Sigma^{X}A^T$. In our case it follows $\Sigma^{X} = Id$, so the equation reduces to $\Sigma^{Y} = AA^T$, where $\Sigma^{Y}$ is given. Additionally, a $\Sigma^{Y}$ matrix is positive definite in all the considered cases. This means that Cholesky decomposition can be used to find a lower triangular matrix $A$ that satisfies $\Sigma^{Y} = AA^T$. The elements of matrix $A$ are given by $A_{ii} = \sqrt{\Sigma^{Y}_{ii} - \sum_{j=0}^{i - 1}A_{ij}^2}$ and $A_{ij} = \frac{1}{A_{j, j}}(\Sigma^{Y}_{jj} - \sum_{k=0}^{j -1}\Sigma^{Y}_{ik}\Sigma^{Y}_{jk})$. This is already implemented in R as a $chol$ method. The matrix $B$ can be calculated directly from the expression $B = \mu^{Y} - A\mu^{X}$. In our scenario it holds $\mu^{X} = 0$, so it follows that B = \mu^{Y}$.

```{r}
mean_matrix <- matrix(c(4.0, 2.0), nrow=2)
covariance_matrix1 <- matrix(c(1.0, 0.9, 0.9, 1.0), nrow=2)
transformed_vectors1 <- transform_vectors_from_standard_norm(vectors, mean_matrix, covariance_matrix1)
plot_vectors(transformed_vectors1, title="Samples from N((4, 2), (1, 0.9, 0.9, 1))")
covariance_matrix2 <- matrix(c(1.0, -0.9, -0.9, 1.0), nrow=2)
transformed_vectors2 <- transform_vectors_from_standard_norm(vectors, mean_matrix, covariance_matrix2)
plot_vectors(transformed_vectors2, title="Samples from N((4, 2), (1, -0.9, -0.9, 1))")
covariance_matrix3 <- matrix(c(9.0, 0.0, 0.0, 1.0), nrow=2)
transformed_vectors3 <- transform_vectors_from_standard_norm(vectors, mean_matrix, covariance_matrix3)
plot_vectors(transformed_vectors3, title="Samples from N((4, 2), (9, 0, 0, 1))")
```

From the plot of samples, it can be seen that variables were transformed to the correct distributions. In the first case, a strong positive correlation between $X_1$ and $Y_1$ is visible. On the other hand, the second plot proves the strong negative correlations between $X_2$ and $Y_2$. On the last plot there is definitely more variance on X axis compared to Y axis. Additionally, all plots are centered in a $(4, 2)^T$ point.


# Task 3

```{r}
plot_samples_variances_covariances <- function(vectors, samples_count) {
  sample_covariance_matrix <- cov(vectors)
  variances <- diag(sample_covariance_matrix)
  hist(
    variances, main = paste("Plot of variances of", samples_count, "samples"),
    xlab = "Variances", breaks = 7, col = "darkorange"
  )
  print(paste0("Mean of variances: ", round(mean(variances), digits=2)))
  covariances <- sample_covariance_matrix[lower.tri(sample_covariance_matrix, diag=FALSE)]
  hist(
    covariances, main = paste("Plot of covariances of", samples_count, "samples"),
    xlab = "Covariances", breaks = 7, col = "darkorange"
  )
  print(paste0("Mean of covariances: ", round(mean(covariances), digits=2)))
}

analyse_vectors_transform <- function(dimension, samples_count) {
  vectors <- matrix(rnorm(n=dimension * samples_count), nrow=samples_count)
  mean_vector <- matrix(rep(0, dimension), nrow=dimension)
  covariance_matrix <- matrix(rep(0.9, dimension * dimension), nrow=dimension)
  diag(covariance_matrix) <- 1
  transformed_vectors <- transform_vectors_from_standard_norm(
    vectors, mean_vector, covariance_matrix
  )
  plot_samples_variances_covariances(transformed_vectors, samples_count)
}

analyse_vectors_transform(dimension = 100, samples_count = 200)
```

From the histogram of the variances of random variables it can be seen that they are concentrated near a value of 1.0. It also follows that a mean of the variances is very close to 1.0 The variance of these estimators is pretty high as only 200 samples were used. The similar phenomenon can be seen on the plot of covariances. The values are close to 0.9, but the variance is high. The mean value is also very close to 0.9.

```{r}
analyse_vectors_transform(dimension = 100, samples_count = 20000)
```

After using 20000 samples, it is more clear that the real value of the variances is equal to 1.0, while the value of covariances is equal to 0.9.
